{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "情感分析rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI0v8WJJy2BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyeHJERM13he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYrGfXmu158R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(dtype=torch.float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE10FQlozye_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNbV-Wga0dBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2ef5393b-386c-458e-b391-45b38b8f9d07"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz:   0%|          | 164k/84.1M [00:00<00:57, 1.47MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 65.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yafHSXiB00NU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6e4eb3b8-1047-4d41-8e12-c826e6b9d050"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YapxbsAm0_e6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7c4e4077-ab2d-4755-ff03-db435a7faaee"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['This', 'movie', 'makes', 'a', 'statement', 'about', 'Joseph', 'Smith', ',', 'what', 'he', 'stood', 'for', ',', 'and', 'what', 'the', 'LDS', 'church', 'believes', '.', 'With', 'all', 'the', 'current', 'media', 'coverage', 'of', 'a', 'certain', 'fugitive', 'people', 'have', 'confused', 'the', 'LDS', 'church', 'with', 'the', 'FLDS', 'church', 'and', 'criminal', 'fugitive', 'Warren', 'Jeffs', '.', 'Jeffs', 'is', 'Not', 'associated', 'with', 'the', 'LDS', 'church', 'yet', 'media', 'groups', 'internationally', 'have', 'asked', 'for', 'comments', 'about', 'Jeffs', 'from', 'The', 'LDS', 'church', '.', 'Jeffs', 'is', 'not', 'mentioned', 'in', 'the', 'movie', 'at', 'all', 'but', 'I', 'think', 'that', 'it', 'is', 'ironic', 'that', 'this', 'movie', 'with', 'all', 'it', \"'s\", 'points', 'about', 'Joseph', 'also', 'point', 'away', 'from', 'the', 'fews', 'of', 'the', 'FLDS', 'church', 'and', 'their', 'leader', 'at', 'this', 'time', 'in', 'the', 'media', 'world', '.', 'This', 'is', 'a', 'movie', 'about', 'Joseph', 'Smith', 'and', 'a', 'great', 'one', 'at', 'that', '.', 'Some', 'of', 'the', 'most', 'obvious', 'differences', 'between', 'Jeffs', 'and', 'Joseph', 'is', 'portrayed', 'in', 'Joseph', \"'s\", 'humanity', ',', 'acceptance', 'and', 'love', '.', 'Jeffs', 'views', 'and', 'opinions', 'differ', 'greatly', 'from', 'Joseph', 'Smith', 'and', 'the', 'LDS', 'Church', 'and', 'it', 'is', 'seen', 'in', 'this', 'movie', '.', 'Jeffs', 'thinks', 'of', 'the', '\"', 'Negro', '\"', 'as', 'devils', '.', 'Joseph', 'Smith', 'knew', 'they', 'were', 'children', 'of', 'god', 'and', 'gave', 'up', 'his', 'wife', \"'s\", 'favorite', 'horse', 'to', 'a', 'African', 'American', '(', 'former', 'slave', ')', 'to', 'buy', 'his', 'son', \"'s\", 'freedom', '.', 'Joseph', 'is', 'shown', 'doing', 'housework', 'for', 'his', 'wife', 'Emma', 'and', 'is', 'criticized', 'by', 'a', 'member', 'until', 'Joseph', 'tells', 'him', 'that', 'a', 'man', 'may', 'lose', 'his', 'wife', 'in', 'the', 'next', 'life', 'if', 'she', 'chooses', 'not', 'to', 'stay', 'with', 'her', 'husband', 'and', 'that', 'doing', 'chores', 'is', 'a', 'way', 'to', 'help', 'and', 'cherish', 'your', 'wife', '.', 'Jeffs', 'brought', 'one', 'of', 'his', 'polygamist', 'wives', 'to', 'her', 'knees', 'in', 'front', 'of', 'a', 'class', 'full', 'of', 'students', 'by', 'grabbing', 'her', 'braid', 'and', 'twisting', 'it', 'painfully', 'till', 'she', 'came', 'to', 'her', 'knees', '.', 'Lastly', 'Joseph', 'participated', 'with', 'law', 'enforcement', 'and', 'sought', 'aid', 'from', 'the', 'government', 'at', 'all', 'times', '.', 'Jeffs', 'thumbs', 'his', 'nose', 'at', 'government', 'and', 'flees', 'at', 'all', 'times.<br', '/><br', '/>I', 'loved', 'this', 'movie', 'and', 'if', 'you', 'do', \"n't\", 'know', 'much', 'about', 'Joseph', 'Smith', 'and', 'what', 'the', 'LDS', 'church', 'believes', ',', 'then', 'this', 'is', 'the', 'movie', 'to', 'see', '.', 'And', 'if', 'you', 'had', 'confused', 'the', 'LDS', 'Church', 'with', 'the', 'FLDS', 'church', 'then', 'you', 'really', 'need', 'to', 'get', 'your', 'act', 'together', '.', 'We', 'are', 'not', 'much', 'different', 'from', 'anyone', 'who', 'believes', 'in', 'Jesus', 'Christ', ',', 'the', 'Sanctity', 'of', 'marriage', 'and', 'the', 'family', ',', 'as', 'well', 'a', 'patriotic', 'to', 'our', 'homeland', 'and', 'country', '.', 'We', 'are', 'all', 'different', 'as', 'well', 'just', 'like', 'you', 'can', 'find', 'different', 'protestants', ',', 'Presbyterians', ',', 'methodist', ',', 'baptist', 'and', 'Catholics', '.', 'What', \"'s\", 'important', 'is', 'our', 'message', 'and', 'what', 'we', 'stand', 'for', '.', 'This', 'movie', 'trys', 'to', 'portray', 'that', 'but', 'there', 'is', 'so', 'much', 'of', 'Joseph', \"'s\", 'life', 'that', 'ca', \"n't\", 'be', 'covered', 'in', 'a', 'mere', '2', 'hour', 'movie', '.', 'This', 'was', 'a', 'really', 'great', 'show', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf5HCKgK1Fnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi0zRfGS1gXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7e7988b-c946-4fa4-bd6b-6d35a1a974ff"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe7wg4Kc1jp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9e3d8cc5-6f90-492b-adb4-f360b181a02d"
      },
      "source": [
        "TEXT.build_vocab(train_data,max_size=25000,vectors='glove.6B.100d',unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                           \n",
            "100%|█████████▉| 399630/400000 [00:14<00:00, 26631.42it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlzEUnHv2N40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1b162eaf-6e85-410a-a20a-09cbd1de96b8"
      },
      "source": [
        "print(len(TEXT.vocab))\n",
        "print(len(LABEL.vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25002\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQpVAVWQ2n5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1c1e0a77-bdc0-4dbb-c773-e3da6f76eeff"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(10))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 203186), (',', 192259), ('.', 166049), ('and', 109479), ('a', 109166), ('of', 101051), ('to', 93837), ('is', 76139), ('in', 61352), ('I', 54641)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX6cs_Tu2yDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4734a054-2714-4061-adc1-4331f63bb8c4"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            "defaultdict(<function _default_unk_index at 0x7ff197be1268>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmW0W5y3L1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUvljwCO3SMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQvZP-hB3g2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data,valid_data,test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA0U3o4j4LSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, padding_idx, hidden_dim, num_layers, bidirectional, dropout, output_dim):\n",
        "    super(RNN,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim,padding_idx=padding_idx)\n",
        "    self.rnn = nn.LSTM(embedding_dim,hidden_dim,num_layers=num_layers,bidirectional=bidirectional,dropout=dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hidden_dim*2,output_dim)\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def forward(self, text, hidden):\n",
        "    # embedded=[sentence_len,batch_size,embedding_dim]\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "    # output=[sentence_len,batch_size,hidden_dim*num_direction]\n",
        "    # hidden=[num_layers*num_direction,batch_size,hidden_dim]\n",
        "    # cell=[num_layers*num_direction,batch_size,hidden_dim]\n",
        "    oupput, (hidden, cell) = self.rnn(embedded, hidden)\n",
        "    #pooled=[batch_size,hidden_dim*num_direction]\n",
        "    pooled = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))\n",
        "    return self.fc(pooled), (hidden, cell)\n",
        "\n",
        "  def init_hidden(self,BATCH_SIZE,requires_grad=True):\n",
        "    weight = next(self.parameters())\n",
        "    return (weight.new_zeros((self.num_layers*2,BATCH_SIZE,self.hidden_dim),requires_grad=requires_grad),\n",
        "        weight.new_zeros((self.num_layers*2,BATCH_SIZE,self.hidden_dim),requires_grad=requires_grad))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ-dF6kS9v3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "padding_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5 \n",
        "output_dim = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVu0jMxf9-V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size, embedding_dim, padding_idx, hidden_dim, num_layers, bidirectional, dropout, output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvJfFaw4Hn9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e324301f-4401-4e10-ac39-983c2bc1555d"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.0259,  0.2626, -0.4150,  ...,  0.2496,  1.0473, -0.8566],\n",
              "        [-0.3556, -0.2554, -0.1192,  ..., -0.1305,  0.1196, -0.4377],\n",
              "        [ 0.1901, -0.2545, -0.3214,  ...,  0.1637,  0.2881, -0.1090]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgIZGVV9_Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i00a4sfIIBQ5",
        "colab_type": "text"
      },
      "source": [
        "训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu_v8mth-JAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion =criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNxOcVzY-fxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  correct = (rounded_preds == y).float()\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIsS1h4KJyK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove this part\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xDhHdyIJH01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "  for batch in iterator:\n",
        "    if len(batch.text[1,:]) == 64:\n",
        "      optimizer.zero_grad()\n",
        "      hidden = repackage_hidden(hidden)\n",
        "      predictions, hidden = model(batch.text,hidden)\n",
        "      predictions = predictions.squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      acc = binary_accuracy(predictions, batch.label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6XkI7TLjXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "  hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      if len(batch.text[1,:]) == 64:\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        predictions, hidden = model(batch.text, hidden)\n",
        "        predictions = predictions.squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "      else:\n",
        "        continue\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWyshwlfOXrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnj9i0FQOX3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_valid_loss = float('inf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3iIv6arM6qP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "46cdb402-1e58-4805-f782-f40dd43e0a1d"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "  train_loss,train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss,valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(),'lstm-model.pt')\n",
        "\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "  print('......')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.672 | Train Acc: 56.99%\n",
            "\t Val. Loss: 0.677 |  Val. Acc: 57.27%\n",
            "......\n",
            "\tTrain Loss: 0.516 | Train Acc: 75.26%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 69.86%\n",
            "......\n",
            "\tTrain Loss: 0.409 | Train Acc: 82.28%\n",
            "\t Val. Loss: 0.506 |  Val. Acc: 72.10%\n",
            "......\n",
            "\tTrain Loss: 0.320 | Train Acc: 86.50%\n",
            "\t Val. Loss: 0.485 |  Val. Acc: 74.62%\n",
            "......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBpuBPliOR4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSJ1I3OGs2Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(sentence):\n",
        "  hidden = model.init_hidden(1)\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  output, hidden = model(tensor,hidden)\n",
        "  prediction = torch.sigmoid(output)\n",
        "  return prediction.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JwjbhHBtsWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4a2239d-e89f-4638-8c0e-3a3fed8216a2"
      },
      "source": [
        "predict_sentiment(\"This film is great\")\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9911580681800842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVylBEztu2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8645fe2a-20d6-40c5-b0f1-fc8071317cae"
      },
      "source": [
        "predict_sentiment(\"This film is terrible\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02054463140666485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w67bWrArxzZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}